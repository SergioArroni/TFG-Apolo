This section will present a summary of the distinct datasets for IDS, along with their corresponding classifiers and
performance metrics.
Our proposed approach will make use of these classifiers.
Furthermore, we will explore the typical types of AML attacks used in this domain.

\subsection{Intrusion Detection Systems datasets}\label{subsec:intrusion-detection-systems-datasets}

IDS datasets play a crucial role in assessing and gauging the performance of IDSs.
These datasets contain labeled instances of regular and anomalous network traffic that are used to train and assess the
precision and efficiency of IDSs.
A range of datasets with diverse strengths and features are accessible.
This section will examine some of the most prevalent datasets, highlighting their essential qualities and applications.

\subsubsection{Discarded datasets}
This project did not make use of several other IDS datasets, including Darpa 1998/99~\cite{darpa1999},
KDD 99~\cite{KDDCUP99}, and NSL-KDD~\cite{KDDCUP99}.
These datasets are no longer commonly employed for evaluating and benchmarking IDS due to their outdated nature.
Created in the late 1990s and early 2000s, they do not accurately represent the current landscape of network threats
and behaviors.
KDD 99 dataset, in particular, has been criticized for its high false positive rate and lack of realism, thereby
limiting its usefulness in assessing the performance of modern IDS~\cite{hugh2000, KDDFaults}.

\subsubsection{CIC-IDS-2017}
A highly utilized IDS dataset in contemporary literature is the CIC-IDS2017~\cite{CICIDS2017}, which was developed
by the Canadian Institute for Cybersecurity (CIC) in a simulated enterprise network environment, gathering network
traffic data for five consecutive days.
This dataset emulates the actions of 25 users and comprises nearly 80 significant attributes~\cite{RING2019147}.
Notably, it has an 83\% to 17\% benign to malicious instance ratio, representing a significant portion of the dataset.
The CIC-IDS2017 is considered an accurate depiction of normal traffic distribution in a network and can be utilized
individually or combined with other datasets~\cite{Shroff2022}.

\subsubsection{CSE-CIC-IDS-2018}
The CSE-CIC-IDS2018 dataset was developed using AWS resources in a simulated enterprise network environment in
2018~\cite{CSE-CIC-IDS2018}.
It consists of data on seven distinct attack categories and comprises nearly 79 important features.
With over 450 devices, including servers, computers, and other tools, this dataset is notably large and
realistic~\cite{pujari2022comparative}.
It is akin to the CIC-IDS2017 dataset, analyzing bidirectional flow packet data, but with more significant features and
greater comprehensiveness.
Hence, it is widely used in the literature for assessing and benchmarking IDSs~\cite{pujari2022comparative}.

\subsubsection{CIC-DDoS-2019}
To address the lack of representation of all DDoS (Distributed Denial of Service) attack subtypes in existing datasets,
the CIC-DDoS-2019 dataset was created~\cite{CICDDoS2019}.
Although the dataset includes simulated network traffic, it strives to present realistic benign data.
It features 13 types of DDoS attacks and over 80 significant features.
However, it is severely imbalanced, with 50,006,249 DDoS attack records and just 56,863 benign traffic records, making
it challenging to train a model on both data types~\cite{RING2019147}.
As a result, experts suggest using this dataset in conjunction with other datasets~\cite{Shroff2022}, such as
CIC-IDS-2017 or CSE-CIC-IDS-2018, to train a more robust model.


\subsection{Intrusion Detection Systems ML-classifiers}\label{subsec:intrusion-detection-systems-classifiers}
ML-classifiers have emerged as a promising alternative to traditional IDS for detecting network attacks.
This is due to the limitations of traditional IDS in dealing with the complex and dynamic nature of cyber-attacks.
In the current digital era, the number and sophistication of malware threats are constantly growing, posing a serious
challenge to network security.
Therefore, it is essential to have reliable and effective IDS systems in place to protect network systems from
potential damage.

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{@{}r|lll|lll|lll|r@{}}
            \cmidrule(lr){2-10}
                                                    & \multicolumn{3}{c|}{\textbf{CIC-IDS-2017}} & \multicolumn{3}{c|}{\textbf{CSE-CIC-IDS-2018}} & \multicolumn{3}{c|}{\textbf{CIC-DDoS-2019}} &                                                                                                                                                                                                                                                     \\ \midrule
            \multicolumn{1}{|l|}{\textbf{Classifiers}} & \textbf{Accuracy}                          & \textbf{F1 Score}                              & \textbf{AUC}                                & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{AUC} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{AUC} & \multicolumn{1}{l|}{\textbf{Ref}}                                                                                                     \\ \midrule
            \multicolumn{1}{|l|}{LR}                   & 92.96                                      & 90.87                                          & 91.50                                       & 87.96             & 88.99             & 81.54        & 91.72             & 87.27             & 90.23        & \multicolumn{1}{l|}{~\cite{cic2019models,cic2018models} }                                                                            \\
            \multicolumn{1}{|l|}{FNN}                  & 99.61                                      & 99.57                                          & 99.83                                       & 93.00             & 92.00             & 100.00       & 95.55             & 95.50             & 95.63        & \multicolumn{1}{l|}{~\cite{huang2020igan, cic2019models, wu2022rtids}}                                                                             \\
            \multicolumn{1}{|l|}{RF}                   & 99.79                                      & 99.78                                          & 99.98                                       & 92.00             & 94.00             & 100.00       & 99.86             & 99.78             & 99.82        & \multicolumn{1}{l|}{~\cite{pujari2022comparative, huang2020igan, Abdulhammed2019, maseer2021benchmarking, faker2019, cic2019models}} \\
            \multicolumn{1}{|l|}{DT}                   & 99.62                                      & 99.57                                          & 99.56                                       & 88.00             & 91.00             & 100.00       & 99.87             & 99.78             & 99.80        & \multicolumn{1}{l|}{~\cite{pujari2022comparative, cic2019models, huang2020igan, maseer2021benchmarking}}                                                                             \\
            \multicolumn{1}{|l|}{RTIDS}                & 99.35                                      & 99.17                                          & 98.83                                       & -                 & -                 & -            & 98.58             & 98.48             & 98.66        & \multicolumn{1}{l|}{~\cite{wu2022rtids}}                                                                             \\
            \multicolumn{1}{|l|}{SVM}                  & 96.97                                      & 96.99                                          & 98.98                                       & 61.00             & 66.00             & 100.00       & 94.02             & 94.98             & 94.24        & \multicolumn{1}{l|}{~\cite{pujari2022comparative, huang2020igan, maseer2021benchmarking, faker2019, wu2022rtids} }                                                                             \\ \bottomrule
        \end{tabular}
    }
    \caption{Performance of the \textit{IDSs} classifiers on the selected datasets.}
    \label{tab:ids-classifiers}
\end{table*}

To achieve this goal, researchers conduct various studies and literature reviews to assess and improve the performance
of ML-based IDS systems.
They also identify potential weaknesses and gaps in existing IDS technologies.

As such, several studies have been conducted to evaluate the performance of various ML-classifiers in detecting network
attacks.
These studies use datasets such as CIC-IDS-2017, CSE-CIC-IDS-2018, and CIC-DDoS-2019.

Table~\ref{tab:ids-classifiers} provides a summary of the most commonly used ML-classifiers and
their scores, including accuracy, F1 Score, and AUC, for each of the aforementioned datasets.
The ML-classifiers used in these studies are \textit{Logistic Regression (LR)}~\cite{wright1995logistic},
\textit{Fuzziness\-based Neural Networks (FNN)}~\cite{ashfaq2017fuzziness}, \textit{Random Forests (RF)}~\cite{cutler2012random},
\textit{Decision Trees (DT)}~\cite{rokach2005decision}, \textit{Robust transformer based Intrusion Detection System (RTIDS)}~\cite{wu2022rtids},
and \textit{Support Vector Machines (SVM)}~\cite{suthaharan2016support}.

accuracy, F1 Score, and AUC are three common metrics used to evaluate the performance of machine learning models.
accuracy measures the proportion of correct predictions made by a model out of the total number of predictions and is
defined as:

\[accuracy = \frac{TP + TN}{TP + TN + FP + FN}\]

where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives,
and FN is the number of false negatives.

F1 Score is a weighted average of precision and the detection rate (DR), where precision measures the proportion of true positives
out of all predicted positives, and the detection rate measures the proportion of true positives out of all actual positives.
The F1 Score is defined as:

\[F1 Score = 2 \cdot \frac{precision \cdot DR}{precision + DR}\]

where precision is defined as:

\[precision = \frac{TP}{TP + FP}\]

and detection rate is defined as:

\[DR = \frac{TP}{TP + FN}\]

AUC, or area under the curve, is a metric used for binary classification problems that measures the overall performance
of a model across different threshold values.
A higher AUC indicates that the model is better at distinguishing between positive and negative classes.
The AUC is calculated by plotting the detection rate against the false positive rate (1 - DR) at various
threshold settings and calculating the area under the resulting curve.

These metrics can be useful in determining the effectiveness of a model and identifying areas for improvement.

Among the classifiers in Table~\ref{tab:ids-classifiers}, \textit{Random Forest}~\cite{zhang2008random} and
\textit{Decision Trees}~\cite{amor2004naive} are found to be some of the most effective classifiers for detecting network attacks.
\textit{Random Forest} has gained popularity due to its ability to handle large datasets and its robust performance even when
the data contains noise or missing values.
\textit{Decision Trees} are also preferred because of their simplicity and interpretability.
They enable clear visualization of the decision-making process, making them useful for understanding the factors that
contribute to the classification results.

Overall, these classifiers have demonstrated strong performance in the field of IDS and are frequently used by
researchers and practitioners.
Their effectiveness in detecting intrusions and classifying network traffic makes them valuable tools for maintaining
the security and integrity of computer networks.

\subsection{Adversarial Machine Learning attacks}\label{subsec:adversarial-machine-learning-attacks}

ML-based IDSs can learn from data and adapt to new situations, unlike traditional systems that rely on predefined rules.
However, ML-based IDSs also face new challenges from attackers who use Artificial Intelligence (AI) to craft
sophisticated attacks that can fool or compromise ML models.
One such threat comes from the use of Artificial Intelligence (AI) in the form of Adversarial Machine Learning (AML),
where attackers use sophisticated techniques to manipulate or subvert ML models.
These attacks are attractive to cyber attackers since they can be challenging to detect and prevent.
Furthermore, as AI techniques gain popularity in cybersecurity, attackers are incentivized to develop more
sophisticated adversarial attacks to evade detection.

Adversarial Machine Learning attacks can be classified as white-box attacks and grey/black-box attacks, depending on
the level of knowledge the attacker possesses about the target model.


\subsubsection{White-box attacks}\label{subsubsec:white-box-attacks}

White-box attacks are the most powerful kind of AML attacks because they let the attacker know everything about the
IDS classifier and the training data.
With this knowledge, the attacker can create very specific and complex attacks that can evade the systemâ€™s defenses
and achieve their goals.
However, white-box attacks are also the most unrealistic kind of attack because they need the attacker to have a lot of
knowledge about the system and its weaknesses, which is often impossible.
In most situations, the attacker will only know some or little about the system, making white-box attacks impossible.
Therefore, white-box attacks are very uncommon in reality and are usually only done in very focused and well-planned
operations.

White-box attacks commonly used on IDS include the \textit{Fast Gradient Sign Method (FGSM)}~\cite{fgsm},
\textit{Deep-Fool}~\cite{deepfool}, \textit{Carlini \&Wagner attack (C\&W)}~\cite{carlini},
\textit{Jacobian based Saliency Map Attack (JSMA)}~\cite{jsma},
\textit{Basic Iterative Method (BIM)}~\cite{bim},
and \textit{Projected Gradient Descent (PGD)}~\cite{pgd}.

\subsubsection{Grey/Black-box attacks}\label{subsubsec:grey-box-attacks}

In contrast to white-box attacks, grey/black-box attacks are more realistic because they do not require the attacker to
have knowledge about the target model.
However, these attacks are often less efficient compared to white-box attacks because the attacker's limited knowledge
about the system restricts their ability to create highly targeted and sophisticated attacks.
Instead, they have to depend on more general techniques that may be less effective in bypassing the system's defenses.

\textit{Generative Adversarial Networks (GANs)}~\cite{goodfellow2020generative} are frequently used in black-box and grey-box
attacks to produce adversarial examples.
\textit{GANs} are a type of Machine Learning model consisting of two neural networks: a generator network and a discriminator
network.
The generator network is trained to produce synthetic data that resembles the real data, while the discriminator
network is trained to differentiate between real and synthetic data.

In the context of black-box and grey-box attacks, the generator network can generate adversarial examples that are
specifically designed to evade the target system's defenses.
The attacker may have access to the system's model scores or just a binary output indicating whether the input was
accepted or rejected.
This information can be utilized to guide the training of the generator network, enhancing its ability to produce
effective adversarial examples.

Recent grey/black-box attacks on IDS include \textit{attackGAN}~\cite{zhao2021attackgan}, \textit{DIGFuPAS}~\cite{duy2021digfupas},
\textit{IDSGAN}~\cite{lin2022idsgan}, \textit{VulnerGAN}~\cite{liu2022vulnergan}, \textit{ZOO attack}~\cite{chen2017zoo},
\textit{Boundary attack}~\cite{chen2019boundary} and the \textit{HotSkipJump attack (HSJA)}~\cite{chen2020hopskipjumpattack}.
\textit{ZOO} is a score-based attack that estimates gradients to create adversarial traffic in grey/black-box settings.
\textit{Boundary attack} and \textit{HSJA} are decision-based attacks that only use binary feedback to craft adversarial inputs.
The \textit{IDSGAN}, \textit{attackGAN}, and \textit{DIGFuPAS} are grey/blackbox attacks that employ \textit{Wasserstein-GAN} to generate adversarial traffic.
\textit{Wasserstein-GAN (W-GAN)}~\cite{gulrajani2017improved} is a \textit{GAN} variant that trains the generator network with a different
objective function called the Wasserstein distance.
The Wasserstein distance measures the distance between two probability distributions and has properties like
smoothness and continuity.
Some recent \textit{WGANs} use the Gradient Penalty to enhance the training convergence.